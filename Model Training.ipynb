{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PEBiUamyILYk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "from skimage.exposure import match_histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKomglqHpOhn"
   },
   "source": [
    "**Implementing Partial Cross-Entropy Loss** - only labeled points would contribute to the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nvGyadGVpNpD"
   },
   "outputs": [],
   "source": [
    "class PartialCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PartialCrossEntropyLoss, self).__init__()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')  # No reduction means we get per-pixel loss\n",
    "\n",
    "    def forward(self, predictions, targets, mask):\n",
    "        # Apply the mask on the pixel-wise loss to only keep labeled points\n",
    "        loss = self.cross_entropy(predictions, targets)  # Per-pixel loss\n",
    "        masked_loss = loss * mask  # Only keep loss for labeled points\n",
    "        return masked_loss.mean()  # Average the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N11AOqZVuZL8"
   },
   "source": [
    "**Dataset Preprocessing** - images and labels in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9BPJKo_iplC9"
   },
   "outputs": [],
   "source": [
    "class LandcoverDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_list = [f for f in os.listdir(data_dir) if '_sat' in f]  # Only take satellite images (jpg)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)  # Number of images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the satellite image and corresponding mask\n",
    "        image_name = self.image_list[idx]\n",
    "        mask_name = image_name.replace('_sat', '_mask').replace('.jpg', '.png')\n",
    "\n",
    "        image_path = os.path.join(self.data_dir, image_name)\n",
    "        mask_path = os.path.join(self.data_dir, mask_name)\n",
    "\n",
    "        # Load the satellite image and mask\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = Image.open(mask_path).convert('RGB')  # Convert label to RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply image transformation (resize, normalize, etc.)\n",
    "\n",
    "        # Resize the label to match the image size (256x256)\n",
    "        label = label.resize((256, 256), Image.NEAREST)  # Resize mask using nearest neighbor (to keep label values)\n",
    "\n",
    "        # Convert RGB labels to class indices using the provided class mapping\n",
    "        label = np.array(label)  # Convert the PIL image to a NumPy array\n",
    "        label = self.rgb_to_class_indices(label)  # Convert RGB mask to class indices\n",
    "\n",
    "        # Convert label to tensor (shape: [256, 256])\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label  # Return the image and 2D label\n",
    "\n",
    "    def rgb_to_class_indices(self, rgb_mask):\n",
    "\n",
    "        \"\"\"Convert an RGB mask to class indices based on predefined colors.\"\"\"\n",
    "\n",
    "        # Define the RGB to class index mapping based on the provided class_dict.csv\n",
    "        rgb_to_class = {\n",
    "            (0, 255, 255): 0,       # Urban Land\n",
    "            (255, 255, 0): 1,       # Agriculture Land\n",
    "            (255, 0, 255): 2,       # Rangeland\n",
    "            (0, 255, 0): 3,         # Forest Land\n",
    "            (0, 0, 255): 4,         # Water\n",
    "            (255, 255, 255): 5,     # Barren Land\n",
    "            (0, 0, 0): 6            # Unknown\n",
    "        }\n",
    "\n",
    "        class_mask = np.zeros((rgb_mask.shape[0], rgb_mask.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        for rgb, class_index in rgb_to_class.items():\n",
    "            match = np.all(rgb_mask == rgb, axis=-1)\n",
    "            class_mask[match] = class_index\n",
    "        return class_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Irj8LyQHwFcU"
   },
   "source": [
    "**Data preprocessing** -resize images and convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5teWfIFsv160"
   },
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Define paths for the dataset directory (train, test, valid directories)\n",
    "train_dir = 'land-cover-classification-dataset/train'\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = LandcoverDataset(train_dir, transform=image_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iz-DVgPH5cLs"
   },
   "source": [
    "**Sample random point labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "k1zoOpIvwdhw"
   },
   "outputs": [],
   "source": [
    "def sample_point_labels(targets, num_points=10000):\n",
    "    mask = torch.zeros_like(targets)  # Create a mask with all zeros\n",
    "    h, w = targets.shape[-2:]  # Get height and width of the label image\n",
    "    indices = torch.randperm(h * w)[:num_points]  # Randomly pick num_points pixels\n",
    "    mask.view(-1)[indices] = 1  # Mark the selected points as labeled\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAnvM_xi5zvs"
   },
   "source": [
    "**Loading pre-trained segmentation model** (DeepLabV3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQjTPJDg5tmm",
    "outputId": "aa3e0886-3c50-4963-90e1-5b35f13875a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ADMIN/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)\n",
    "model.classifier[4] = nn.Conv2d(256, 7, kernel_size=1)  # Adjust model for 7 classes (based on your dataset)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "criterion = PartialCrossEntropyLoss()  # Use our custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyP50Jd66RAE"
   },
   "source": [
    "**Training model with point labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Et2PATNf6Bx5"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, epochs=22, num_points=10000):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, targets in dataloader:\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "            masks = torch.stack([sample_point_labels(target) for target in targets])  # Create masks for labeled points\n",
    "            outputs = model(images)['out']  # Forward pass (get model predictions)\n",
    "\n",
    "            # Ensure the outputs have the shape (batch_size, num_classes, H, W)\n",
    "            loss = criterion(outputs, targets, masks)  # Calculate loss based on labeled points\n",
    "\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update model weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBQo2CMu62Zd"
   },
   "source": [
    "**Run the model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5-FLis2Y65wH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.21456626516122085\n",
      "Epoch 2, Loss: 0.1720361623626489\n",
      "Epoch 3, Loss: 0.15826412233022544\n",
      "Epoch 4, Loss: 0.14930987033324364\n",
      "Epoch 5, Loss: 0.1497581056677378\n",
      "Epoch 6, Loss: 0.14935774776415947\n",
      "Epoch 7, Loss: 0.13795044254033995\n",
      "Epoch 8, Loss: 0.13834679642548928\n",
      "Epoch 9, Loss: 0.13270225452306944\n",
      "Epoch 10, Loss: 0.12733492981164884\n",
      "Epoch 11, Loss: 0.12507808762483108\n",
      "Epoch 12, Loss: 0.12011546049362574\n",
      "Epoch 13, Loss: 0.12418570082921249\n",
      "Epoch 14, Loss: 0.12350879752865204\n",
      "Epoch 15, Loss: 0.10909216086833905\n",
      "Epoch 16, Loss: 0.11320377007508889\n",
      "Epoch 17, Loss: 0.11199842288325994\n",
      "Epoch 18, Loss: 0.1201382380647537\n",
      "Epoch 19, Loss: 0.11169733231266339\n",
      "Epoch 20, Loss: 0.10709816026381958\n",
      "Epoch 21, Loss: 0.10782228161891301\n",
      "Epoch 22, Loss: 0.10534701744715373\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hBhDF6T4Gsdv"
   },
   "outputs": [],
   "source": [
    "# After training, save the model\n",
    "torch.save(model, 'trained_landcover_model_2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model_weights_2.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
